<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Is this Math?</title>
    <link>/</link>
    <description>Recent content on Is this Math?</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Tue, 22 Sep 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Tangent vectors as derivations</title>
      <link>/2020/09/22/tangent-vectors-as-derivations/</link>
      <pubDate>Tue, 22 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/09/22/tangent-vectors-as-derivations/</guid>
      <description>\[\require{physics}\]
Notes on an Introduction to Manifolds (by Tu)Some extremely wordy notes based on An Introduction to Manifolds by Loring W. Tu, mainly to clarify some basis things to myself.
A key concept is coordinate-free objects. In this case, how do we define tangent vectors in a coordinate free way? (But also why do we even care?)
Tangent vectors in \(\mathbb{R}^n\) as DerivationsThe book starts with defining tangent vectors to a curve (in \(\mathbb{R}^3\)) at point \(p\).</description>
    </item>
    
    <item>
      <title>Setting up WSL2 with R and Visual Studio Code</title>
      <link>/2020/08/13/setting-up-wsl2-with-r-and-visual-studio-code/</link>
      <pubDate>Thu, 13 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/13/setting-up-wsl2-with-r-and-visual-studio-code/</guid>
      <description>This is mostly to document the resources I used to set up the Windows Subsystem for Linux 2 (WSL/2) for a local installation of R on Linux as well as setting up allowing Visual Studio Code (vscode) to remote into a machine with R installed and get nice features such as linting and intellisense.
Setting up WSL2 and Windows TerminalWSL2
How I did this will probably be outdated so I’ll link to the docs instead:</description>
    </item>
    
    <item>
      <title>Some notation for extended likelihoods</title>
      <link>/2020/08/03/some-notation-for-extended-likelihoods/</link>
      <pubDate>Mon, 03 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/03/some-notation-for-extended-likelihoods/</guid>
      <description>Some notation of extended likelihoodsIn classical likelihood theory we have this “duality” between the data-generating view and the likelihood view:
\(f_\theta(y)\) gives the probability of generating a \(y\) for fixed \(\theta\)\(L_y(\theta)\) gives the likelihood of \(\theta\) for fixed \(y\)We can extend this analogy to the extended likelihood which incorporates random effects. This duality gives a Bayesian-esque flavor (mathematically, not philosophically).
Characters:
\(\theta\) – fixed parameters\(v\) – random parameters\(f(\cdot)\) – assorted set of probability distributionsData Generation:Random:\[f_\theta(v)\]</description>
    </item>
    
    <item>
      <title>A not markov chain example</title>
      <link>/2020/08/02/not-a-markov-chain/</link>
      <pubDate>Sun, 02 Aug 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/08/02/not-a-markov-chain/</guid>
      <description>Showing a stochastic process is not a Markov chain by exampleThe definition of a Markov chain states that a sequence \((X_n)\) is a MC if the following holds:
\[P(X_{n+1}|X_n, \dots X_0) = P(X_{n+1}|X_n)\]
for all \(x\) values 1That is the probability of what happens in the next step only depends on the previous step. Or put another way, there’s no additional information in \((X_{n-1}, \dots, X_0)\) so “knowing”/conditioning on it does not change the probability of \(P(X_{n+1}|X_n)\).</description>
    </item>
    
    <item>
      <title>Axiomatically, the frequentist likelihood is invariant under parameter transformation</title>
      <link>/2020/07/27/axiomatically-the-frequentist-likelihood-is-invariant-under-parameter-transformation/</link>
      <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/07/27/axiomatically-the-frequentist-likelihood-is-invariant-under-parameter-transformation/</guid>
      <description>Axiomatically, the frequentist likelihood is invariant under parameter transformationThat our inference be invariant under 1-1 data transformation seems like a natural requirement and follows from the definition of the likelihood. That is if we transform the data, we transform the pdf and thus have a new likelihood.
It seems reasonable to require that the likelihood ratio be invariant under fixed parameter transformation as well: regardless if I estimate \(\sigma^2\) or \(\sigma\) I should have the same amount of information.</description>
    </item>
    
    <item>
      <title>Transforming the data of a likelihood involves a Jacobian; why the likelihood ratio is the key object</title>
      <link>/2020/07/27/transforming-the-data-of-a-likelihood-involves-a-jacobian/</link>
      <pubDate>Mon, 27 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/07/27/transforming-the-data-of-a-likelihood-involves-a-jacobian/</guid>
      <description>The likelihood ratio is the primary object of interest because if we transform the data \(X\) to \(Y\), our information about a fixed parameter \(\theta\) should not change. This mathematically follows from calculus.
Transforming the data (random variable) of a likelihood involves a Jacobian term\[L(\theta; y) = L(\theta; x)\left| \frac{\partial x}{\partial y}\right|\]
The [[Classical definition of the likelihood]] starts with a statistical model and thus a probability density \(p_\theta(x)\).</description>
    </item>
    
    <item>
      <title>Model-assisted frameworks: a hybrid between design-based and model-based frameworks</title>
      <link>/2020/07/21/model-assisted-frameworks-a-hybrid-between-design-based-and-model-based-frameworks/</link>
      <pubDate>Tue, 21 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/07/21/model-assisted-frameworks-a-hybrid-between-design-based-and-model-based-frameworks/</guid>
      <description>To overcome the limitations of both the pure model-based and pure design-based frameworks, a hybrid model-assisted framework has been developed. In this framework a statistical model is specified to determine the parameters of interest but estimation and inference of said parameters is done under a design-based framework.
For example consider a model fit on the finite population of size \(N\) as
\[y_i = \beta_0 + \beta_1x_i\]
then the finite population quantity of interest are \(\beta_0\) and \(\beta_1\) values that minimizes that squared error:</description>
    </item>
    
    <item>
      <title>Design-based vs Model-based framework</title>
      <link>/2020/07/20/design-based-vs-model-based-framework/</link>
      <pubDate>Mon, 20 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/07/20/design-based-vs-model-based-framework/</guid>
      <description>The postulate of randomness thus resolves itself into the question, “Of what population is this a random sample?” which must frequently be asked by every practical statistician. (Fisher 1922)
When we say our sample set is a sample from a population, what population are we talking about?
In Fisher’s model-based framework, you begin with specifying a statistical model which specifies a data generating process for your \(y_i\)’s. Now imagine we generate and store \(y_i\)’s ad infinitum.</description>
    </item>
    
    <item>
      <title>A higher level examination of M-estimator consistency proof</title>
      <link>/2020/07/10/a-higher-level-examination-of-m-estimator-consistency-proof/</link>
      <pubDate>Fri, 10 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/07/10/a-higher-level-examination-of-m-estimator-consistency-proof/</guid>
      <description>The proof of the [[Consistency of M-Estimators (uniform convergence)]] has two major parts:
converging the “\(y\)” values: by squeezing \(M(\theta_0)\) in between \(M(\hat{\theta}_n)\) and \(M_n(\hat{\theta}_n)\)“\(y\)” values only get close near \(\theta_0\): well separated says that the only place where \(M\) is nearConverging the \(y\) valuesI was originally imagining the first part of the proof using a full \(M(\cdot)\) by \(\theta\) graph but it might be easier to just imagine it as a straight line.</description>
    </item>
    
    <item>
      <title>Walking through the delta method</title>
      <link>/2020/07/06/walking-through-the-delta-method/</link>
      <pubDate>Mon, 06 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/07/06/walking-through-the-delta-method/</guid>
      <description>Imagine we’ve constructed an estimator \(T_n\) for some parameter \(\theta\) but what we really want to know about is \(\phi(\theta)\), some transformation of the parameter.
Can we use our knowledge of the behavior of \(T_n\) to learn about \(\phi(T_n)\)?
By the continuous-mapping theorem if \(\phi\) is continuous then:
if \(T_n \to \theta\)then \(\phi(T_n) \to \phi(\theta)\)By the delta method we have
if \(\sqrt{n}(T_n - \theta) \rightsquigarrow T\) as \(n \to \infty\)then \(\sqrt{n}(\phi(T_n) - \phi(\theta)) \rightsquigarrow \phi&amp;#39;_\theta(T)\)where \(\phi&amp;#39;_\theta(T) = \phi&amp;#39;(\theta)*T\).</description>
    </item>
    
    <item>
      <title>O notation depends on what limit you&#39;re taking</title>
      <link>/2020/07/02/o-notation-depends-on-what-limit-you-re-taking/</link>
      <pubDate>Thu, 02 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/07/02/o-notation-depends-on-what-limit-you-re-taking/</guid>
      <description>Something that confused me about \(O\) notation was that it appeared to take different meaning between algorithms and calculus. This was resolved by seeing that the notation provides a convenient way of describing the limit of a function \(f(x)\) as \(x\) approaches a specific limit. This point about the \(O\)-ness of a function being contingent on what limit you were taking was the crux of my confusion.
This limit is typically different between a standard algorithms and calculus course.</description>
    </item>
    
    <item>
      <title>Calculating the determinent of an equicovariance matrix</title>
      <link>/2020/06/30/calculating-the-determinent-of-an-equicovariance-matrix/</link>
      <pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/06/30/calculating-the-determinent-of-an-equicovariance-matrix/</guid>
      <description>The goal is to find the determinant by re-writing \(\Sigma\) as a upper triangular matrix. Then [[Determinant of triangular matrix is the product of the diagonal]]
We add a new row and new column. Recall that the determinant can be defined recursively. Since the new matrix has a \(1\) in the \(\Sigma_{11}\) position, then the determinant remains the same.We aim to eliminate the original off diagonals. We need to remove the lower diagonal for obvious reasons.</description>
    </item>
    
    <item>
      <title>WIP - Does convergence in measure imply convergence a.s.?</title>
      <link>/2020/06/30/does-convergence-in-probability-imply-convergence-a-s/</link>
      <pubDate>Tue, 30 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/2020/06/30/does-convergence-in-probability-imply-convergence-a-s/</guid>
      <description>We define \(X_i = [a,b] := X_i = I_{[0,1]}/(b-a)\) the indicator RV over \([0,1]\) normalized such that it integrates to 1.
\[\begin{aligned} X_1 &amp;amp; = [0,1] \\ X_2, X_3 &amp;amp; = [0,1/2), [1/2,1] \\ X_4,X_5,X_6 &amp;amp; = [0,1/3), [1/3,2/3), [2/3,1] \\ \dots \end{aligned}\]
That is we can imagine a box that is shrinking in width while increasing in height.</description>
    </item>
    
    <item>
      <title>About</title>
      <link>/about/</link>
      <pubDate>Thu, 05 May 2016 21:48:51 -0700</pubDate>
      
      <guid>/about/</guid>
      <description>This is a &amp;ldquo;hello world&amp;rdquo; example website for the blogdown package. The theme was forked from @jrutheiser/hugo-lithium-theme and modified by Yihui Xie.</description>
    </item>
    
  </channel>
</rss>